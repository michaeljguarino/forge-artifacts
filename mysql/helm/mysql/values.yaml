oidcProxy:
  enabled: false
  upstream: http://localhost:3000
  issuer: https://oidc.plural.sh/
  clientID: ""
  clientSecret: ""
  cookieSecret: ""
  ingress:
    host: mysql-orchestrator.david.onplural.sh

mysql-operator:
  replicas: 1
  image:
    repository: dkr.plural.sh/mysql/bitpoke/mysql-operator
    tag: v0.6.1
    imagePullPolicy: Always
  sidecar57:
    image:
      repository: dkr.plural.sh/mysql/bitpoke/mysql-operator-sidecar-5.7
      tag: v0.6.1
  sidecar80:
    image:
      repository: dkr.plural.sh/mysql/bitpoke/mysql-operator-sidecar-8.0
      tag: v0.6.1

  # Insert a pre-stop lifecycle hook and trigger a failover. NOTE: Use this when your cluster network
  # policy allows to connect across namespaces and the mysql node is able to connecto to operator pod
  gracefulShutdown:
    enabled: false

  imagePullSecrets:
  # - name: "image-pull-secret"

  # whether or not to install CRDs
  installCRDs: true
  # in which namespace to watch for resource, leave empty to watch in all namespaces
  watchNamespace:

  # The operator will install a ServiceMonitor if you have prometheus-operator installed.
  serviceMonitor:
    enabled: true
    ## Additional labels for the serviceMonitor. Useful if you have multiple prometheus operators running to select only specific ServiceMonitors
    # additionalLabels:
    #   prometheus: prom-internal
    interval: 10s
    scrapeTimeout: 3s
    # jobLabel:
    # targetLabels:
    # podTargetLabels:
    namespaceSelector:
      any: true
    selector:
      matchLabels:
        app.kubernetes.io/managed-by: mysql.presslabs.org
        app.kubernetes.io/name: mysql

  podLabels: {}
  podAnnotations: {}

  nodeSelector: {}
  tolerations: []
  resources:
    requests:
     cpu: 10m
     memory: 72Mi

  ## nodeAffinity settings
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: cloud.google.com/gke-preemptible
  #         operator: NotIn
  #         values:
  #         - true

  ## Anti-Affinity setting. The default "hard" will use pod anti-affinity that is
  ## requiredDuringSchedulingIgnoredDuringExecution to ensure 2 services don't
  ## end up on the same node. Setting this to "soft" will use
  ## preferredDuringSchedulingIgnoredDuringExecution. If set to anything else,
  ## no anti-affinity rules will be configured.
  antiAffinity: "none"

  extraArgs: []

  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1

  podSecurityPolicy:
    enabled: false
    annotations:
      seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
      seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'

  rbac:
    create: true
    # if rbac is false this service account is used
    serviceAccountName: default

  orchestrator:
    image:
      repository: dkr.plural.sh/mysql/bitpoke/mysql-operator-orchestrator
      tag: v0.6.1

    # orchestrator user and password to manage MySQL clusters
    topologyUser: orchestrator
    topologyPassword:  # this is empty and will be random generated if not specified
    # secretName:  # you can specify which secret to use for orchestrator topology credentials

    resources:
      requests:
        cpu: 50m
        memory: 30Mi

    service:
      type: ClusterIP
      port: 80
      # nodePort: 3000

    ingress:
      enabled: false
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      hosts:
      - host: chart-example.local
        paths: []

      tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local

    persistence:
      enabled: true
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"
      accessMode: "ReadWriteOnce"
      size: 1Gi

    # key value map of orchestrator conf directives.
    # see: https://github.com/github/orchestrator/blob/master/conf/orchestrator-sample.conf.json
    # the following keys are manages and thus cannot be overwritten:
    #   - ListenAddress :3000
    #   - MySQLTopologyCredentialsConfigFile /orchestrator/conf/orc-topology.cnf
    #   - BackendDB sqlite
    #   - SQLite3DataFile /var/lib/orchestrator/orc.db
    #   - RaftEnabled true
    #   - RaftDataDir /var/lib/orchestrator
    #   - RaftBind $HOSTNAME
    #   - RaftNodes The statefullset members
    config:
      Debug: false
      # the operator is handling the registries, do not auto discover
      DiscoverByShowSlaveHosts: false
      # forget missing instances automatically
      UnseenInstanceForgetHours: 1

      InstancePollSeconds: 5
      HostnameResolveMethod: "none"
      MySQLHostnameResolveMethod: "@@report_host"
      RemoveTextFromHostnameDisplay: ":3306"
      DetectClusterAliasQuery: "SELECT CONCAT(SUBSTRING(@@hostname, 1, LENGTH(@@hostname) - 1 - LENGTH(SUBSTRING_INDEX(@@hostname,'-',-2))),'.',SUBSTRING_INDEX(@@report_host,'.',-1))"
      DetectInstanceAliasQuery: "SELECT @@hostname"
      SlaveLagQuery: "SELECT TIMESTAMPDIFF(SECOND,ts,UTC_TIMESTAMP()) as drift FROM sys_operator.heartbeat ORDER BY drift ASC LIMIT 1"

      # Automated recovery (this is opt-in, so we need to set these)
      # Prevent recovery flip-flop, by disabling auto-recovery for 5 minutes per
      # cluster
      RecoveryPeriodBlockSeconds: 300
      # Do not ignore any host for auto-recovery
      RecoveryIgnoreHostnameFilters: []
      # Recover both, masters and intermediate masters
      RecoverMasterClusterFilters: ['.*']
      RecoverIntermediateMasterClusterFilters: ['.*']
      # `reset slave all` and `set read_only=0` on promoted master
      ApplyMySQLPromotionAfterMasterFailover: true
      # https://github.com/github/orchestrator/blob/master/docs/configuration-recovery.md#promotion-actions
      # Safety! do not disable unless you know what you are doing
      FailMasterPromotionIfSQLThreadNotUpToDate: true
      DetachLostReplicasAfterMasterFailover: true
      # set downtime on the failed master
      MasterFailoverLostInstancesDowntimeMinutes: 10

      # orchestrator hooks called in the following order
      # for more information about template: https://github.com/github/orchestrator/blob/master/go/logic/topology_recovery.go#L256
      ProcessesShellCommand: "sh"

      OnFailureDetectionProcesses: 
        - "/usr/local/bin/orc-helper event -w '{failureClusterAlias}' 'OrcFailureDetection' 'Failure: {failureType}, failed host: {failedHost}, lost replcas: {lostReplicas}' || true"
        - "/usr/local/bin/orc-helper failover-in-progress '{failureClusterAlias}' '{failureDescription}' || true"

      # PreGracefulTakeoverProcesses:
      PreFailoverProcesses:
        # as backup in case the first request fails
        - "/usr/local/bin/orc-helper failover-in-progress '{failureClusterAlias}' '{failureDescription}' || true"
      # PostFailoverProcesses:
      #   - "/usr/local/bin/orchestrator-helper event '{failureClusterAlias}' 'Orc{command}' 'Failure type: {failureType}, failed hosts: {failedHost}, slaves: {countSlaves}' || true"

      PostUnsuccessfulFailoverProcesses:
        - "/usr/local/bin/orc-helper event -w '{failureClusterAlias}' 'OrcPostUnsuccessfulFailover' 'Failure: {failureType}, failed host: {failedHost} with {countSlaves} slaves' || true"

      PostMasterFailoverProcesses:
        - "/usr/local/bin/orc-helper event '{failureClusterAlias}' 'OrcPostMasterFailover' 'Failure type: {failureType}, new master: {successorHost}, slaves: {slaveHosts}' || true"

      PostIntermediateMasterFailoverProcesses:
        - "/usr/local/bin/orc-helper event '{failureClusterAlias}' 'OrcPostIntermediateMasterFailover' 'Failure type: {failureType}, failed hosts: {failedHost}, slaves: {countSlaves}' || true"

      # PostGracefulTakeoverProcesses:
