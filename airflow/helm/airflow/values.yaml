# Default values for airflow.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

sshConfig:
  enabled: false
  id_rsa: dummy
  id_rsa_pub: dummy
httpConfig:
  enabled: false
  username: dummy
  password: dummy

secrets:
  redis_password: example

test-base:
  enabled: false
  repository: airflow
  testName: airflow-integration
  promoteTag: warm

postgres:
  enabled: true
  team: plural
  user: airflow
  dbName: airflow
  ownerChart: airflow
  infix: ""
  parameters:
    max_connections: '101'

exporter:
  image:
    repository: dkr.plural.sh/airflow/prom/statsd-exporter
    tag: v0.22.8
    pullPolicy: IfNotPresent
  
  podSecurityContext: {}

  securityContext: {}

  replicas: 1

  service:
    type: ClusterIP
    annotations: {}

  web:
    # The address on which to expose the web interface and generated Prometheus metrics.
    port: 9102

    # Path under which to expose metrics.
    path: /metrics

  serviceMonitor:
    interval: 30s
    scrapeTimeout: 10s
    additionalLabels: {}

  statsd:
    relayAddress: null
    # The UDP port on which to receive statsd metric lines.
    udpPort: 9125

    # The TCP port on which to receive statsd metric lines.
    tcpPort: 9125

    # Maximum size of your metric mapping cache.
    # Relies on least recently used replacement policy if max size is reached.
    cacheSize: 1000

    # Size of internal queue for processing events.
    eventQueueSize: 10000

    # Number of events to hold in queue before flushing.
    eventFlushThreshold: 1000

    # Time interval before flushing events in queue.
    eventFlushInterval: 200ms

    # Metric mapping configuration
    mappingConfig: |-

config-overlays:
  configOverlays:
    airflow-registry:
      spec:
        folder: server
        subfolder: image
        name: Airflow Registry
        documentation: docker repository to use for airflow (default is dkr.plural.sh/airflow/apache/airflow)
        updates:
        - path: ['airflow', 'airflow', 'airflow', 'image', 'repository']
    airflow-tag:
      spec:
        folder: server
        subfolder: image
        name: Airflow Image Tag
        documentation: docker repository tag to use for airflow (default is 2.1.4-python3.8-dbt-2)
        updates:
        - path: ['airflow', 'airflow', 'airflow', 'image', 'tag']
    airflow-statsd-host:
      spec:
        folder: server
        subfolder: metrics
        name: statsd host
        documentation: the hostname to publish statsd metrics, we default this to a prometheus statsd proxy but for datadog would be eg datadog.datadog
        updates:
        - path: ['airflow', 'airflow', 'airflow', 'config', 'AIRFLOW__METRICS__STATSD_HOST']
    airflow-statsd-relay:
      spec:
        folder: server
        subfolder: metrics
        name: statsd relay
        documentation: a hostname to relay statsd metrics to, this will preserve prometheus monitoring while also enabling additional metrics exports (default is datadog.datadog:8125)
        updates:
        - path: ['airflow', 'exporter', 'statsd', 'relayAddress']
    airflow-statsd-port:
      spec:
        folder: server
        subfolder: metrics
        name: statsd port
        documentation: the port to publish statsd metrics, datadog listens on 8125
        updates:
        - path: ['airflow', 'airflow', 'airflow', 'config', 'AIRFLOW__METRICS__STATSD_PORT']
    web-replicas:
      spec:
        folder: server
        subfolder: scaling
        name: Web Replicas
        documentation: number of replicas for the web deployment
        updates:
        - path: ['airflow', 'airflow', 'web', 'replicas']
        inputType: int
    worker-replicas:
      spec:
        folder: server
        subfolder: scaling
        name: Worker Replicas
        documentation: number of replicas for the worker deployment
        updates:
        - path: ['airflow', 'airflow', 'workers', 'replicas']
        inputType: int
    web-cpu:
      labels:
        platform.plural.sh/component: airflow-web
        platform.plural.sh/kind: deployment
        platform.plural.sh/resource: cpu
      spec:
        name: web cpu
        documentation: cpu requests for web deployment
        updates:
        - path: ['airflow', 'airflow', 'web', 'resources', 'requests', 'cpu']
    web-mem:
      labels:
        platform.plural.sh/component: airflow-web
        platform.plural.sh/kind: deployment
        platform.plural.sh/resource: memory
      spec:
        name: web memory
        documentation: memory requests for web deployment
        updates:
        - path: ['airflow', 'airflow', 'web', 'resources', 'requests', 'memory']
    worker-cpu:
      labels:
        platform.plural.sh/component: airflow-worker
        platform.plural.sh/kind: statefulset
        platform.plural.sh/resource: cpu
      spec:
        name: worker cpu
        documentation: cpu requests for worker statefulset
        updates:
        - path: ['airflow', 'airflow', 'workers', 'resources', 'requests', 'cpu']
    worker-mem:
      labels:
        platform.plural.sh/component: airflow-worker
        platform.plural.sh/kind: statefulset
        platform.plural.sh/resource: memory
      spec:
        name: worker memory
        documentation: memory requests for worker statefulset
        updates:
        - path: ['airflow', 'airflow', 'workers', 'resources', 'requests', 'memory']
    scheduler-cpu:
      labels:
        platform.plural.sh/component: airflow-scheduler
        platform.plural.sh/kind: deployment
        platform.plural.sh/resource: cpu
      spec:
        name: scheduler cpu
        documentation: cpu requests for scheduler deployment
        updates:
        - path: ['airflow', 'airflow', 'scheduler', 'resources', 'requests', 'cpu']
    scheduler-mem:
      labels:
        platform.plural.sh/component: airflow-scheduler
        platform.plural.sh/kind: deployment
        platform.plural.sh/resource: memory
      spec:
        name: scheduler memory
        documentation: memory requests for scheduler deployment
        updates:
        - path: ['airflow', 'airflow', 'scheduler', 'resources', 'requests', 'memory']

airflow:
  externalDatabase:
    type: postgres
    host: plural-airflow
    port: 5432
    database: airflow
    user: airflow
    passwordSecret: airflow.plural-airflow.credentials.postgresql.acid.zalan.do
    passwordSecretKey: password

    # use this for any extra connection-string settings, e.g. ?sslmode=disable
    properties: "?sslmode=allow"

  postgresql:
    enabled: false
    image:
      registry: gcr.io
      repository: pluralsh/postgres
      tag: 11.7.0-debian-10-r9
    metrics:
      enabled: true
      image:
        registry: gcr.io
        repository: pluralsh/postgres-exporter
        tag: 0.8.0
      serviceMonitor:
        enabled: true
    existingSecret: airflow-postgres-password
  
  dags:
    gitSync:
      # image:
      #   uid: 65533
      #   gid: 65533
      resources:
        requests:
          cpu: 50m
          memory: 64Mi

      # syncTimeout: 1000

  pgbouncer:
    enabled: false

  prometheusRule:
    enabled: true
  serviceMonitor:
    enabled: true
  
  flower:
    enabled: false
  
  rbac:
    create: true
  
  serviceAccount:
    name: airflow
  
  redis:
    image:
      registry: gcr.io/pluralsh
      tag: 6.0.9-debian-10-r0
    enabled: true
    existingSecret: airflow-redis-password
    existingSecretPasswordKey: redis-password

    metrics:
      enabled: true
      image:
        registry: gcr.io/pluralsh
        tag: 1.12.1-debian-10-r11
      serviceMonitor:
        enabled: true

    cluster:
      enabled: false
      slaveCount: 1

    master:
      resources:
        requests:
          cpu: "10m"
          memory: "32Mi"

      persistence:
        enabled: false

    slave:
      resources:
        requests:
          cpu: "10m"
          memory: "32Mi"

      persistence:
        enabled: false

  airflow:
    executor: CeleryExecutor
    image:
      repository: dkr.plural.sh/airflow/apache/airflow
      tag: 2.1.4-python3.8-dbt-2

    config:
      ## security
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost/
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "True"

      ## dags
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "30"

      ## remote log storage
      AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
      AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "plural"

      AIRFLOW__METRICS__STATSD_ON: "True"
      AIRFLOW__METRICS__STATSD_HOST: "statsd-exporter"
      AIRFLOW__METRICS__STATSD_PORT: "9125"
      AIRFLOW__METRICS__STATSD_PREFIX: "airflow"
    
    usersUpdate: false

    variables:
    - key: "environment"
      value: "prod"
    
    pools:
    - name: "pool_1"
      slots: 5
      description: "example pool with 5 slots"
    - name: "pool_2"
      slots: 10
      description: "example pool with 10 slots"

  ingress:
    enabled: true
    apiVersion: networking.k8s.io/v1
    web:
      tls:
        enabled: true
        secretName: airflow-tls
      annotations:
        kubernetes.io/tls-acme: "true"
        kubernetes.io/ingress.class: "nginx"
        cert-manager.io/cluster-issuer: letsencrypt-prod
        nginx.ingress.kubernetes.io/force-ssl-redirect: 'true'
        nginx.ingress.kubernetes.io/use-regex: "true"
  web:
    replicas: 1

    webserverConfig:
      ## the full text value to mount as the webserver_config.py file
      ##
      stringOverride: |-
        from flask_appbuilder.security.manager import AUTH_DB
        # use embedded DB for auth
        AUTH_TYPE = AUTH_DB

    resources:
      requests:
        cpu: "100m"
        memory: "500Mi"

  scheduler:
    livenessProbe: 
      timeoutSeconds: 60
  
  workers:
    ## if the airflow workers StatefulSet should be deployed
    ##
    enabled: true

    replicas: 2

    ## resource requests/limits for the airflow worker Pods
    ##
    resources:
      requests:
        cpu: "100m"
        memory: "500Mi"

    podDisruptionBudget:
      enabled: true

      maxUnavailable: "20%"

    autoscaling:
      enabled: true
      maxReplicas: 8
      metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 80
    celery:
      ## if we should wait for tasks to finish before SIGTERM of the celery worker
      ##
      gracefullTermination: true

      ## how many seconds to wait for tasks to finish before SIGTERM of the celery worker
      ##
      ## WARNING:
      ## - GKE cluster-autoscaler will not respect graceful termination period over 10min
      ## NOTE:
      ## - this gives any running tasks AT MOST 9min to complete
      ##
      gracefullTerminationPeriod: 540

    ## how many seconds to wait after SIGTERM before SIGKILL of the celery worker
    ##
    terminationPeriod: 60
  
  logs:
    ## configs for the logs PVC
    ##
    persistence:
      ## if a persistent volume is mounted at `logs.path`
      ##
      enabled: false
